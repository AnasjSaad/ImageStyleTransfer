# -*- coding: utf-8 -*-
"""ImageStyleTransfer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-sPrsfTJ1uZrDAWx7R3_knkBXCtJxihX
"""

from google.colab import drive
from google.colab.patches import cv2_imshow
import os

drive.mount('/content/drive')

os.chdir('/content/drive/My Drive/Projects/Image Style Transfer')

#Defining Paths and importing libraries

import tensorflow as tf
import numpy as np
from tensorflow.python.keras import models
import os 
import glob
from PIL import Image
from tensorflow.python.keras.preprocessing import image as tf_image
import matplotlib.pyplot as plt
import cv2

#paths
content_path='/content/drive/My Drive/Projects/Image Style Transfer/Content/Inputs/renee.jpg'
style_path='/content/drive/My Drive/Projects/Image Style Transfer/Content/Inputs/starry.jpg'
Lion='/content/drive/My Drive/Projects/Image Style Transfer/Content/Inputs/Lion.jpg'

"""**Part 1: Image Utilities**


We will be definign a couple of functions for reading images, preproccesing them to use for the VGG19 network and deprocessing then saving the image

**Function 1: Load Image**
"""

#Load Image

def load_img(img_path):
  #read the image and convert the computation grapgh to an image format
  img=tf.io.read_file(img_path)
  img=tf.image.decode_image(img, channels=3)
  img=tf.image.convert_image_dtype(img,tf.float32)

  shape=tf.cast(tf.shape(img)[:-1], tf.float32)

  max_dim=1024

  max_side=max(shape)
  scale=max_dim/max_side
  new_shape=tf.cast(shape*scale,tf.int32)
  img=tf.image.resize(img, new_shape, method=tf.image.ResizeMethod.BILINEAR)

  img=img[tf.newaxis,:]
  return img

"""**EXAMPLE 1: Load Image**"""

img_ex=tf.io.read_file(content_path)
img_ex.shape

#Decode Input Tensor
img_ex=tf.image.decode_image(img_ex, channels=3)
img_ex=tf.image.convert_image_dtype(img_ex,tf.float32)

#Display the image
plt.imshow(img_ex)
img_ex.shape

#get width and height of the image 
shape=tf.cast(tf.shape(img_ex)[:-1],tf.float32)
shape

#Setting max dimension of the image
max_dim=512
#check which side is longer, this will be used to generate the scale
max_side=max(shape)
scale=max_dim/max_side
print('Your max side is', np.array(max_side, dtype=np.uint32), 
      'pixels long so it will be scaled down to', max_dim, 'pixels.')
new_shape=tf.cast(shape*scale,tf.int32)
img_ex_a=tf.image.resize(img_ex,new_shape)
print()

#Display the Image
plt.imshow(img_ex_a)
print('The new dimension of our image is', np.array(new_shape, dtype=np.uint32))

#we musut now add an extra dimension in the front so it becomes a 4D object that
#the VGG19 network can take in
img_ex_b = img_ex_a[tf.newaxis, :]

print('Our tensor has the dimensions:', img_ex_b.shape)

"""**Function 2: Deprocess Image**"""

def deprocess_img(processed_img):
  processed_img = processed_img*255
  processed_img = np.array(processed_img, dtype=np.uint8)
  if np.ndim(processed_img)>3:
    assert processed_img.shape[0] == 1
    processed_img = processed_img[0]
  return Image.fromarray(processed_img)

"""**Function 3: Save Image**"""

def save_img(best_img, path):
  img = Image.fromarray(best_img)
  img.save(path)

"""**Function 4: Modified imshow**"""

def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

img_ex2 = load_img(content_path)
img_ex2_deproc = deprocess_img(img_ex2)
plt.imshow(img_ex2_deproc)

#taking a look at the network we will be using
vgg= tf.keras.applications.VGG19(include_top=True, weights='imagenet')

for layer in vgg.layers:
  print(layer.name)

"""**THE VGG19 NETWORK**"""

import tensorflow as tf
import numpy as np
from tensorflow.python.keras import models

#defining layers to be used for each model

content_layers = ['block5_conv2']
style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1',
                'block4_conv1',
                'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

#Function will load model 

def get_vggLayers(style_or_content_layers):

  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

#already trained off imagenet, we are just essentially setting the inputs accurately.
  vgg.trainable = False

#outputs are set to the layers specified, either style or content
  vgg_input = [vgg.input]
  vgg_output = [vgg.get_layer(name).output for name in style_or_content_layers]
  model = tf.keras.Model(vgg_input, vgg_output)
  return model

"""**Creating Model**"""

# Load an image
style_img_ex4 = load_img(style_path)

# Extract the layers needed
style_extractor_ex4 = get_vggLayers(style_layers)

# Find the outputs
style_outputs_ex4 = style_extractor_ex4(style_img_ex4*255)

# Look at the stats of each layer's output
for name, output in zip(style_layers, style_outputs_ex4):
  print(name)
  print(" Shape: ", output.numpy().shape)
  print(" Min: ", output.numpy().min())
  print(" Max: ", output.numpy().max())
  print(" Mean: ", output.numpy().mean())
  print()

"""We know that works now we test the content path."""

# Load an image
content_img_ex4 = load_img(content_path)
# Extract the layers needed
content_extractor_ex4 = get_vggLayers(content_layers)
# Find the outputs
content_outputs_ex4 = content_extractor_ex4(content_img_ex4*255)
# Look at the stats of each layer's output
for name, output in zip(content_layers, content_outputs_ex4):
  print(name)
  print(" shape: ", output.numpy().shape)
  print(" min: ", output.numpy().min())
  print(" max: ", output.numpy().max())
  print(" mean: ", output.numpy().mean())
  print()

def gram_matrix(input_tensor):
  """
  Generates the Gram matrix representation of the style features. This function takes an
  input tensor and will apply the proper steps to generate the Gram matrix
  """
  # Equation (3)
  # Generate image channels. If the input tensor is a 3D array of size Nh x Nw x Nc, reshape
  # it to a 2D array of Nc x (Nh*Nw). The shape[-1] takes the last element in the shape
  # characterstic, that being the number of channels. This will be our second dimension
  channels = int(input_tensor.shape[-1])
  print(channels)
  # Reshape the tensor into a 2D matrix to prepare for Gram matrix calculation by multiplying
  # all of the dimensions except the last one (which is what the -1 represents) together
  Fl_ik = tf.reshape(input_tensor, [-1, channels])
  # Transpose the new 2D matrix
  Fl_jk = tf.transpose(Fl_ik)
  # Find all the elements in the new array (Nw*Nh)
  n = tf.shape(Fl_ik)[0]
  # Perform the Gram matrix calculation
  gram = tf.matmul(Fl_jk, Fl_ik)/tf.cast(n, tf.float32)
  # Generate the Gram matrix as a tensor for use in our model
  gram_tensor = gram[tf.newaxis, :]
  return gram_tensor

#tensorflow has its own implimentation
def gram_matrix_(input_tensor):
  summation = tf.linalg.einsum('bijc, bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)
  result = summation/num_locations
  return result

"""**Example of the Gram Matrix**"""

# Load an input image as an example tensor
style_img_ex5 = load_img(style_path)
# Sanity check
print(gram_matrix(style_img_ex5))
#tensor flows own implimentation
print(gram_matrix_(style_img_ex5))

"""**Style and Content Feature Extraction**"""

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg = get_vggLayers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False


  def call(self, inputs):
    # Expects a float input between [0, 1]
    inputs = inputs * 255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])
    style_outputs = [gram_matrix(style_output) for style_output in style_outputs]
    content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)}
    style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}
    
    return{'content':content_dict, 'style':style_dict}


feature_extractor = StyleContentModel(style_layers, content_layers)

"""**Test Style and Content Feature Extraction**"""

# Load an image
content_img_ex6 = load_img(content_path)

# A Tensorflow constant is different from a variable. A constant type makes it so that when you declare it, the actual value of it can't be changed
results_ex6 = feature_extractor(tf.constant(content_img_ex6))

print('Styles:')
for name, output in sorted(results_ex6['style'].items()):
  print(" ", name)
  print(" shape: ", output.numpy().shape)
  print(" min: ", output.numpy().min())
  print(" max: ", output.numpy().max())
  print(" mean: ", output.numpy().mean())
  print()

print("Contents:")
for name, output in sorted(results_ex6['content'].items()):
  print(" ", name)
  print(" shape: ", output.numpy().shape)
  print(" min: ", output.numpy().min())
  print(" max: ", output.numpy().max())
  print(" mean: ", output.numpy().mean())

"""**Claculating Gradient Descent and Loss Function**"""

# Load the style and content images
style_img = load_img(style_path)
content_img = load_img(content_path)

# Extract the features from the style and content images
style_features = feature_extractor(style_img)['style']
content_features = feature_extractor(content_img)['content']

generated_img = tf.Variable(content_img)

# Remember that all of our images are read in as a float32 type, so we have to define the range as [0, 1] to keep it within 255
def clip_range(img):
  return tf.clip_by_value(img, clip_value_min=0.0, clip_value_max=1.0)

  # Choose an optimizer.
optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-8)

# Remember that we are trying to optimize the Total Loss function. However, there are values that correspond to the style and content weight
# The Alpha value corresponds to content weight and the Beta corresponds to the style weight. Let's define these
alpha = 1e4
beta = 45

# Now we define our loss functions
def style_content_loss(outputs):
  style_outputs = outputs['style']
  content_outputs = outputs['content']

  # Define the style loss function
  style_loss = tf.add_n([tf.reduce_mean(tf.square(style_outputs[name] - style_features[name])) for name in style_outputs.keys()])
  # Multiply the style loss by the weighted variable to get the weighted style loss
  style_loss *= beta / num_style_layers

  # Define the content loss function
  content_loss = tf.add_n([tf.reduce_mean(tf.square(content_outputs[name] - content_features[name])) for name in content_outputs.keys()])
  # Multiply the content loss by the weighted variable to get the weighted content loss
  content_loss *= alpha / num_content_layers

  # Define the total loss function
  total_loss = style_loss + content_loss
  return total_loss

"""**Ensure GPU is utilized**"""

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# State the total variational weight
total_variational_weight= 30
@tf.function()
def train_step(img):
  # Tensorflow's GradientTape function performs automatic differentiation of the input
  with tf.GradientTape() as tape:
    outputs = feature_extractor(img)
    loss = style_content_loss(outputs)
    loss += total_variational_weight * tf.image.total_variation(img)
  # Apply the gradient by passing the loss and the generated image
  grad = tape.gradient(loss, img)
  # The gradient is optimized using the ADAM optimizer we declared earlier. This optimizes the
  # generated image using the gradient values
  optimizer.apply_gradients([(grad, img)])
  # The image is rewritten, being sure that the values all stay within the viable range of [0, 1]
  img.assign(clip_range(img))

import time
from IPython import display
# Start the run time
start = time.time()
epochs = 100
iterations = 100
step = 0
for n in range(epochs):
  for m in range(iterations):
    step += 1
    train_step(generated_img)
    print(".", end=' ')
  display.clear_output(wait=True)
  display.display(deprocess_img(generated_img))
  print("Train step: {}".format(step))
end = time.time()
print("Total Time: {:.1f}".format(end-start))
# Save Image Files
file_name = '/content/drive/My Drive/Projects/Image Style Transfer/Content/Outputs/image.png'
deprocess_img(generated_img).save(file_name)